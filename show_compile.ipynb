{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b19d62f",
   "metadata": {},
   "source": [
    "# torch.compile的不同mode，dynamic设置的差异展示\n",
    "PyTorch 2.x 以后引入了一个新的，强大的功能————torch.compile。它是一个简单，易用的能对pytorch模型进行编译优化的函数，在这里本人（笨人）试图展示一下compile这个函数不同的参数（dynamic，mode）设置对应的不同效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7c28f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# 库引入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781f6d0",
   "metadata": {},
   "source": [
    "## dynamic？True or False?\n",
    "dynamic，意味动态的，这个参数是一个bool类型的参数，可设置为True或False，它实际上指的是，我在优化中是按照“输入是否可变”为标准去进行不同的优化方案。\n",
    "\n",
    "举个例子，当我的输入固定为（B,C,H,W），每个维度都指定固定的值，比如B=32，C=3，H=64，W=64。那么这种情况下我的输入就是固定的，是一种“静态的”。\n",
    "\n",
    "但如果，我的某个参数可能发生改变，比如B有时是32，有时是64，那么这个时候我的输入就是不定的，是“动态的”。\n",
    "\n",
    "显而易见，前者最好用dynamic=False去compile，后者最好用dynamic=True去compile。\n",
    "\n",
    "那么这两种参数设置方式有什么优缺点呢。\n",
    "\n",
    "对于dynamic=True的情况下，优点在于我们用一种动态的方式去编译，它允许我们的可变输入和可变控制流，限制较为宽松。缺点在于性能略差一些。\n",
    "\n",
    "对于dynamic=False的情况下，优点在于编译后的运行性能更好，但是缺点在于输入不可变，可能不支持可变控制流，输入一但变化，可能就要重新编译，引发新的编译耗时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bae1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的小网络，以便后边进行展示\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(512, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(512, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd337fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "静态编译模型前向传播用时0.3697 ms\n",
      "动态编译模型前向传播用时1.0393 ms\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet().to(\"cuda\")  # 将模型放到GPU上\n",
    "static_compile = torch.compile(net, dynamic=False)\n",
    "dynamic_compile = torch.compile(net, dynamic=True)\n",
    "# 这里的dynamic_compile和static_compile是两个不同的编译版本\n",
    "# 下边我们用随机数做一个伪输入\n",
    "input_32 = torch.randn(32, 512).to(\"cuda\")  # 输入数据也放到GPU上\n",
    "input_64 = torch.randn(64, 512).to(\"cuda\")  # 这里我们也可以准备一个批次为64的输入\n",
    "# 预热（预热这里其实有很重要的概念，稍后会讲解，这里可以先不在意这个地方）\n",
    "_ = static_compile(input_32)\n",
    "_ = dynamic_compile(input_32) \n",
    "# 先不使用批次为64的输入，我们来看输入不变的情况下，使用dynamic与否 编译后模型性能的差异\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "# 进行一次前向计算\n",
    "_ = static_compile(input_32)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize() # 等待所有CUDA操作完成\n",
    "inference_time_static = start_event.elapsed_time(end_event)\n",
    "print(f\"静态编译模型前向传播用时{inference_time_static:.4f} ms\")\n",
    "\n",
    "start_event.record()\n",
    "_ = dynamic_compile(input_32)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize() # 等待所有CUDA操作完成\n",
    "inference_time_dynamic = start_event.elapsed_time(end_event)\n",
    "print(f\"动态编译模型前向传播用时{inference_time_dynamic:.4f} ms\")\n",
    "# 这里从结果我们可以看出，静态编译的模型前向传播用时更少\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3410b0e",
   "metadata": {},
   "source": [
    "这里需要打断一下了，一个很有意思的点在于，我们刚刚提到dynamic=False是静态去编译，dynamic=True是动态去编译，但是，我们似乎在调用compile函数本身的时候，并没有显式指定我们的输入的形状，换言之，调用compile的时候，模型并不知道它要静态编译为输入batchsize为32的情况。那么它是怎么知道它要为batch size 32 去编译的呢。\n",
    "\n",
    "这里就和预热有关系，我们在调用compile的时候，其实并没有完全对这个函数/模型去进行编译，而是配置了相关的编译环境，编译参数。当我们第一次运行这个模型（在上边就是预热），它会依据输入去进行实际的编译（lazy compile）。也就是说，调用了compile后模型的首次运行，包含了编译时间和推理时间两部分，第二次及以后调用，就只有推理时间了。\n",
    "\n",
    "我们刚刚运行过（预热过），所以static的编译版本现在已经固定了为32的batch size去编译优化。我们把输入换成64的batch size去输入，看看会发生什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a4d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "静态编译模型前向传播用时1.2831 ms\n",
      "动态编译模型前向传播用时0.5662 ms\n"
     ]
    }
   ],
   "source": [
    "# 进行一次前向计算\n",
    "start_event.record()\n",
    "_ = static_compile(input_64)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize() # 等待所有CUDA操作完成\n",
    "inference_time_static = start_event.elapsed_time(end_event)\n",
    "print(f\"静态编译模型前向传播用时{inference_time_static:.4f} ms\")\n",
    "\n",
    "start_event.record()\n",
    "_ = dynamic_compile(input_64)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize() # 等待所有CUDA操作完成\n",
    "inference_time_dynamic = start_event.elapsed_time(end_event)\n",
    "print(f\"动态编译模型前向传播用时{inference_time_dynamic:.4f} ms\")\n",
    "# 这里从结果我们可以看出，静态编译的模型前向传播用时更少"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ac9c6",
   "metadata": {},
   "source": [
    "不难发现，如果我们违背了静态长度编译的使用条件（长度不变），我们就会强制打破编译的条件，这时我们会触发静态编译的重新编译，导致它花费更长的时间去推理（因为里边包含了重新编译的时间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09210912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无编译模型前向传播用时1.2370 ms\n",
      "动态编译模型前向传播用时0.7288 ms\n"
     ]
    }
   ],
   "source": [
    "# 这里再补充一个对比，我们看一下编译前后模型的性能差异\n",
    "start_event.record()\n",
    "_ = net(input_32)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize() # 等待所有CUDA操作完成\n",
    "inference_time_static = start_event.elapsed_time(end_event)\n",
    "print(f\"无编译模型前向传播用时{inference_time_static:.4f} ms\")\n",
    "\n",
    "start_event.record()\n",
    "_ = dynamic_compile(input_32)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize() # 等待所有CUDA操作完成\n",
    "inference_time_dynamic = start_event.elapsed_time(end_event)\n",
    "print(f\"动态编译模型前向传播用时{inference_time_dynamic:.4f} ms\")\n",
    "# 我们发现即便是和编译效果稍差的动态编译对比，他们之间的性能差距都是非常显著的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5550cca",
   "metadata": {},
   "source": [
    "## mode\n",
    "这里介绍三种主要的mode，\n",
    "default，reduce-overhead，max-autotune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c207ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小的模型结果不是很稳定，这里叠甲用个大一点的模型\n",
    "# PS: claude直接给我设计了一个挺复杂的模型,我们不需要在意这个模型的具体架构\n",
    "class ComplexBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bottleneck_ratio=0.5):\n",
    "        super().__init__()\n",
    "        bottleneck_channels = int(out_channels * bottleneck_ratio)\n",
    "        \n",
    "        # 主路径\n",
    "        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
    "        \n",
    "        # 多分支结构\n",
    "        self.branch1 = nn.Conv2d(bottleneck_channels, bottleneck_channels//4, 3, padding=1)\n",
    "        self.branch2 = nn.Conv2d(bottleneck_channels, bottleneck_channels//4, 5, padding=2)\n",
    "        self.branch3 = nn.Conv2d(bottleneck_channels, bottleneck_channels//4, 7, padding=3)\n",
    "        self.branch4 = nn.Conv2d(bottleneck_channels, bottleneck_channels//4, 1)\n",
    "        \n",
    "        # 分组卷积\n",
    "        self.group_conv = nn.Conv2d(bottleneck_channels, bottleneck_channels, \n",
    "                                   kernel_size=3, padding=1, groups=4)\n",
    "        \n",
    "        # SE注意力块\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(bottleneck_channels, bottleneck_channels//16, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(bottleneck_channels//16, bottleneck_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 输出投影\n",
    "        self.conv2 = nn.Conv2d(bottleneck_channels, out_channels, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # 残差连接\n",
    "        self.skip = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 主路径\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # 多分支\n",
    "        b1 = self.branch1(out)\n",
    "        b2 = self.branch2(out)\n",
    "        b3 = self.branch3(out)\n",
    "        b4 = self.branch4(out)\n",
    "        \n",
    "        # 合并分支\n",
    "        out = torch.cat([b1, b2, b3, b4], dim=1)\n",
    "        \n",
    "        # 分组卷积\n",
    "        out = F.relu(self.group_conv(out))\n",
    "        \n",
    "        # SE注意力\n",
    "        se_weight = self.se(out)\n",
    "        out = out * se_weight\n",
    "        \n",
    "        # 输出投影\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # 残差连接\n",
    "        out += self.skip(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 初始卷积层\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # 复杂块堆叠\n",
    "        self.block1 = ComplexBlock(64, 128)\n",
    "        self.block2 = ComplexBlock(128, 256)\n",
    "        self.block3 = ComplexBlock(256, 512)\n",
    "        self.block4 = ComplexBlock(512, 1024)\n",
    "        \n",
    "        # 高级特征处理\n",
    "        self.context_block = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 全局池化和分类器\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入预处理和特征提取\n",
    "        x = self.stem(x)\n",
    "        \n",
    "        # 复杂块级联\n",
    "        x = self.block1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.block2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.block3(x)\n",
    "        x = F.max_pool2d(x, 2)  \n",
    "        x = self.block4(x)\n",
    "        \n",
    "        # 上下文建模\n",
    "        x = self.context_block(x)\n",
    "        \n",
    "        # 全局特征和分类\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "input_tensor = torch.randn(32, 3, 224, 224).to(\"cuda\") \n",
    "complex_net = ComplexModel().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5327829",
   "metadata": {},
   "source": [
    "### default\n",
    "默认模式，编译时间短，性能适中，适合大多数的使用场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5857f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预热编译用时133613.0156 ms\n",
      "default模式下编译后的模型前向传播用时8.4707 ms\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "compile_time_list = []\n",
    "compiled_net = torch.compile(complex_net, mode='default')\n",
    "start_event.record()\n",
    "_ = compiled_net(input_tensor)  # 预热编译\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "warm_time = start_event.elapsed_time(end_event)\n",
    "print(f\"预热编译用时{warm_time:.4f} ms\")\n",
    "# 下边是default模式下编译后的模型前向传播的用时测试\n",
    "for i in range(n):\n",
    "    start_event.record()\n",
    "    _ = compiled_net(input_tensor)\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "    compile_time = start_event.elapsed_time(end_event)\n",
    "    compile_time_list.append(compile_time)\n",
    "print(f\"default模式下编译后的模型前向传播用时{sum(compile_time_list) / n:.4f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e954eb",
   "metadata": {},
   "source": [
    "### reduce-overhead\n",
    "使用 CUDA Graphs 减少 Python 和 GPU 的交互开销，适合小模型或低延迟推理\n",
    "限制在于：\n",
    "CUDA Graphs 不支持动态形状（除非显式配置）。\n",
    "内存使用略高（存储 Graph 状态）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "419394b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预热编译用时108352.1016 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py:2345: UserWarning: Unable to hit fast path of CUDAGraphs because of pending, uninvoked backwards. Consider running with torch.no_grad() or using torch.compiler.cudagraph_mark_step_begin() before each model invocation\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce-overhead模式下编译后的模型前向传播用时10.6827 ms\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "compile_time_list = []\n",
    "compiled_net = torch.compile(complex_net, mode='reduce-overhead')\n",
    "start_event.record()\n",
    "_ = compiled_net(input_tensor)  # 预热编译\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "warm_time = start_event.elapsed_time(end_event)\n",
    "print(f\"预热编译用时{warm_time:.4f} ms\")\n",
    "# 下边是default模式下编译后的模型前向传播的用时测试\n",
    "for i in range(n):\n",
    "    start_event.record()\n",
    "    _ = compiled_net(input_tensor)\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "    compile_time = start_event.elapsed_time(end_event)\n",
    "    compile_time_list.append(compile_time)\n",
    "print(f\"reduce-overhead模式下编译后的模型前向传播用时{sum(compile_time_list) / n:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab0112",
   "metadata": {},
   "source": [
    "### max-autotune\n",
    "通过性能分析选择最佳内核配置，追求最大性能，适合大模型或高吞吐量场景\n",
    "编译时间最长，性能最优，适合大模型。适合静态长度编译。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34bbb540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTOTUNE convolution(32x3x224x224, 64x3x7x7)\n",
      "  convolution 0.2755 ms 100.0% \n",
      "  triton_convolution2d_3 0.5868 ms 46.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_1 0.5888 ms 46.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_4 0.6069 ms 45.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_0 0.6267 ms 44.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_5 0.7124 ms 38.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_2 0.8694 ms 31.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=2, STRIDE_W=2, UNROLL=False, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.8576 seconds and 0.0015 seconds precompiling\n",
      "AUTOTUNE convolution(32x64x56x56, 128x64x1x1)\n",
      "  triton_convolution2d_46 0.0911 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_48 0.0911 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  convolution 0.0922 ms 98.9% \n",
      "  triton_convolution2d_43 0.0932 ms 97.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_44 0.1014 ms 89.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_47 0.1023 ms 89.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_49 0.1024 ms 89.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_45 0.1106 ms 82.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.3407 ms 26.7% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9812 seconds and 0.0017 seconds precompiling\n",
      "AUTOTUNE convolution(32x64x56x56, 64x64x1x1)\n",
      "  convolution 0.0614 ms 100.0% \n",
      "  triton_convolution2d_10 0.0616 ms 99.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_11 0.0625 ms 98.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_6 0.0645 ms 95.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_9 0.0645 ms 95.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_7 0.0655 ms 93.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_12 0.0655 ms 93.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_8 0.0799 ms 76.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.2499 ms 24.6% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9612 seconds and 0.0018 seconds precompiling\n",
      "AUTOTUNE convolution(32x128x28x28, 256x128x1x1)\n",
      "  convolution 0.0535 ms 100.0% \n",
      "  triton_convolution2d_100 0.0563 ms 94.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_104 0.0584 ms 91.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_103 0.0596 ms 89.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_101 0.0604 ms 88.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_105 0.0614 ms 87.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_106 0.0614 ms 87.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_102 0.0901 ms 59.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.3040 ms 17.6% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9691 seconds and 0.0018 seconds precompiling\n",
      "AUTOTUNE convolution(32x128x28x28, 128x128x1x1)\n",
      "  triton_convolution2d_58 0.0338 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_57 0.0381 ms 88.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_61 0.0389 ms 86.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_62 0.0389 ms 86.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  convolution 0.0399 ms 84.6% \n",
      "  triton_convolution2d_63 0.0420 ms 80.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_60 0.0428 ms 79.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_59 0.0563 ms 60.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.1802 ms 18.8% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9658 seconds and 0.0017 seconds precompiling\n",
      "AUTOTUNE convolution(32x256x14x14, 512x256x1x1)\n",
      "  triton_convolution2d_159 0.0399 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_158 0.0410 ms 97.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_162 0.0438 ms 91.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  convolution 0.0461 ms 86.7% \n",
      "  triton_convolution2d_164 0.0521 ms 76.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_161 0.0543 ms 73.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_163 0.0592 ms 67.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_160 0.0891 ms 44.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.3482 ms 11.5% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9693 seconds and 0.0021 seconds precompiling\n",
      "AUTOTUNE convolution(32x256x14x14, 256x256x1x1)\n",
      "  convolution 0.0276 ms 100.0% \n",
      "  triton_convolution2d_118 0.0300 ms 92.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_120 0.0327 ms 84.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_114 0.0346 ms 79.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_117 0.0358 ms 77.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_115 0.0369 ms 75.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_119 0.0389 ms 71.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_116 0.0573 ms 48.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.1915 ms 14.4% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9628 seconds and 0.0017 seconds precompiling\n",
      "AUTOTUNE convolution(32x512x7x7, 1024x512x1x1)\n",
      "  triton_convolution2d_221 0.0430 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_223 0.0522 ms 82.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  convolution 0.0532 ms 80.8% \n",
      "  triton_convolution2d_222 0.0561 ms 76.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_218 0.0563 ms 76.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_220 0.0565 ms 76.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_217 0.0603 ms 71.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_219 0.0993 ms 43.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.3297 ms 13.0% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9720 seconds and 0.0021 seconds precompiling\n",
      "AUTOTUNE convolution(32x512x7x7, 512x512x1x1)\n",
      "  convolution 0.0310 ms 100.0% \n",
      "  triton_convolution2d_176 0.0358 ms 86.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_178 0.0522 ms 59.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_175 0.0543 ms 57.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_177 0.0543 ms 57.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_173 0.0563 ms 55.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_172 0.0584 ms 53.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_174 0.0942 ms 32.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.1781 ms 17.4% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9653 seconds and 0.0018 seconds precompiling\n",
      "AUTOTUNE convolution(32x1024x7x7, 512x1024x1x1)\n",
      "  convolution 0.0512 ms 100.0% \n",
      "  triton_convolution2d_235 0.0563 ms 90.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_237 0.0840 ms 61.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_234 0.0881 ms 58.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_236 0.0901 ms 56.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_232 0.0933 ms 54.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_231 0.1014 ms 50.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_233 0.1690 ms 30.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.3226 ms 15.9% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9815 seconds and 0.0020 seconds precompiling\n",
      "AUTOTUNE convolution(32x32x1x1, 512x32x1x1)\n",
      "  triton_convolution2d_214 0.0041 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_216 0.0041 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_212 0.0042 ms 97.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=2\n",
      "  triton_convolution2d_213 0.0049 ms 83.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_211 0.0050 ms 82.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  convolution 0.0051 ms 80.0% \n",
      "  triton_convolution2d_215 0.0053 ms 77.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_210 0.0060 ms 67.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  conv1x1_via_mm 0.2898 ms 1.4% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9546 seconds and 0.0016 seconds precompiling\n",
      "AUTOTUNE convolution(32x16x1x1, 256x16x1x1)\n",
      "  triton_convolution2d_153 0.0040 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_154 0.0041 ms 97.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=2\n",
      "  triton_convolution2d_155 0.0041 ms 97.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_157 0.0041 ms 97.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_152 0.0043 ms 92.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  convolution 0.0051 ms 78.1% \n",
      "  triton_convolution2d_156 0.0051 ms 78.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=256, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  conv1x1_via_mm 0.0061 ms 65.1% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.8550 seconds and 0.0013 seconds precompiling\n",
      "E0529 10:18:37.898000 3755 site-packages/torch/_inductor/select_algorithm.py:1300] [2/4] Exception out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/l6/cl6encfut4q6qj6vnzkl6iwdnf73cuisc6ckhxxekr6fkb5yiz3h.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4)\n",
      "E0529 10:18:37.899000 3755 site-packages/torch/_inductor/select_algorithm.py:1300] [2/4] Exception out of resource: shared memory, Required: 245760, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/uj/cuj6u7hwctus5zv4mpzjdihm4jh46jng5egp3oaujygrh7np6pqw.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4)\n",
      "E0529 10:18:37.900000 3755 site-packages/torch/_inductor/select_algorithm.py:1300] [2/4] Exception out of resource: shared memory, Required: 163840, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/t3/ct3egbhleryc4urr3wq4wee77f4j77uexfe7tek5tjjujnf2e7tx.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8)\n",
      "E0529 10:18:37.901000 3755 site-packages/torch/_inductor/select_algorithm.py:1300] [2/4] Exception out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/3i/c3iw5t6fkmcpjl3xiu2evpaetf5qzdlflvaqgvhfq37vjnmyjkcu.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4)\n",
      "W0529 10:18:38.436000 3755 site-packages/torch/_inductor/select_algorithm.py:1517] [2/4] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n",
      "W0529 10:18:38.756000 3755 site-packages/torch/_inductor/select_algorithm.py:1517] [2/4] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n",
      "W0529 10:18:39.085000 3755 site-packages/torch/_inductor/select_algorithm.py:1517] [2/4] out of resource: shared memory, Required: 245760, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n",
      "W0529 10:18:39.406000 3755 site-packages/torch/_inductor/select_algorithm.py:1517] [2/4] out of resource: shared memory, Required: 163840, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n",
      "AUTOTUNE mm(32x512, 512x256)\n",
      "  mm 0.0092 ms 100.0% \n",
      "  triton_mm_241 0.0195 ms 47.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_mm_239 0.0196 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_251 0.0307 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_238 0.0308 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2\n",
      "  triton_mm_245 0.0317 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8\n",
      "  triton_mm_240 0.0328 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_mm_252 0.0328 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_mm_249 0.0351 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_247 0.0358 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.5027 seconds and 0.0085 seconds precompiling\n",
      "AUTOTUNE convolution(32x8x1x1, 128x8x1x1)\n",
      "  triton_convolution2d_95 0.0041 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_96 0.0041 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_97 0.0041 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=2\n",
      "  triton_convolution2d_98 0.0041 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_99 0.0041 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  convolution 0.0049 ms 83.7% \n",
      "  conv1x1_via_mm 0.0059 ms 69.2% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.7534 seconds and 0.0011 seconds precompiling\n",
      "AUTOTUNE convolution(32x4x1x1, 64x4x1x1)\n",
      "  triton_convolution2d_41 0.0031 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=2\n",
      "  triton_convolution2d_42 0.0033 ms 94.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_40 0.0041 ms 76.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  convolution 0.0049 ms 63.6% \n",
      "  conv1x1_via_mm 0.0053 ms 59.0% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.5414 seconds and 0.0007 seconds precompiling\n",
      "AUTOTUNE convolution(32x512x1x1, 32x512x1x1)\n",
      "  conv1x1_via_mm 0.0063 ms 100.0% \n",
      "  convolution 0.0072 ms 87.9% \n",
      "  triton_convolution2d_209 0.0102 ms 61.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_207 0.0154 ms 41.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_208 0.0154 ms 41.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=2\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.5123 seconds and 0.0009 seconds precompiling\n",
      "AUTOTUNE convolution(32x256x1x1, 16x256x1x1)\n",
      "  conv1x1_via_mm 0.0061 ms 100.0% \n",
      "  triton_convolution2d_151 0.0072 ms 85.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=2\n",
      "  convolution 0.0073 ms 84.6% \n",
      "  triton_convolution2d_149 0.0092 ms 66.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=2\n",
      "  triton_convolution2d_150 0.0102 ms 60.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=2\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.5351 seconds and 0.0006 seconds precompiling\n",
      "AUTOTUNE convolution(32x128x1x1, 8x128x1x1)\n",
      "  triton_convolution2d_94 0.0051 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=2\n",
      "  convolution 0.0061 ms 83.3% \n",
      "  conv1x1_via_mm 0.0061 ms 83.3% \n",
      "  triton_convolution2d_92 0.0061 ms 83.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=2\n",
      "  triton_convolution2d_93 0.0061 ms 83.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=2\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.4816 seconds and 0.0007 seconds precompiling\n",
      "AUTOTUNE convolution(32x64x1x1, 4x64x1x1)\n",
      "  triton_convolution2d_39 0.0041 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=2\n",
      "  triton_convolution2d_37 0.0044 ms 92.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=2\n",
      "  convolution 0.0051 ms 80.0% \n",
      "  triton_convolution2d_38 0.0051 ms 80.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=2\n",
      "  conv1x1_via_mm 0.0061 ms 66.7% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.5422 seconds and 0.0013 seconds precompiling\n",
      "AUTOTUNE convolution(32x64x56x56, 16x64x3x3)\n",
      "  triton_convolution2d_17 0.1137 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_18 0.1137 ms 99.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_16 0.1147 ms 99.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_13 0.1229 ms 92.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_14 0.1242 ms 91.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_15 0.1261 ms 90.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "  convolution 0.1495 ms 76.0% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.7681 seconds and 0.0020 seconds precompiling\n",
      "AUTOTUNE convolution(32x64x56x56, 16x64x5x5)\n",
      "  convolution 0.2284 ms 100.0% \n",
      "  triton_convolution2d_24 0.2724 ms 83.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_20 0.2785 ms 82.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_21 0.3018 ms 75.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "  triton_convolution2d_22 0.3398 ms 67.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=16, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_19 0.3973 ms 57.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_23 0.5325 ms 42.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.8240 seconds and 0.0026 seconds precompiling\n",
      "AUTOTUNE convolution(32x64x56x56, 16x64x7x7)\n",
      "  convolution 0.3757 ms 100.0% \n",
      "  triton_convolution2d_30 0.5417 ms 69.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_26 0.5437 ms 69.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_27 0.5646 ms 66.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "  triton_convolution2d_28 0.7393 ms 50.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_25 1.1358 ms 33.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_29 1.2615 ms 29.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.8649 seconds and 0.0021 seconds precompiling\n",
      "AUTOTUNE convolution(32x64x56x56, 16x64x1x1)\n",
      "  triton_convolution2d_32 0.0430 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_34 0.0440 ms 97.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_35 0.0440 ms 97.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_31 0.0451 ms 95.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_36 0.0451 ms 95.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_33 0.0461 ms 93.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  convolution 0.0502 ms 85.7% \n",
      "  conv1x1_via_mm 0.1720 ms 25.0% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.8484 seconds and 0.0016 seconds precompiling\n",
      "AUTOTUNE convolution(32x128x28x28, 32x128x3x3)\n",
      "  convolution 0.0471 ms 100.0% \n",
      "  triton_convolution2d_67 0.0891 ms 52.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=32, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_70 0.0908 ms 51.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=32, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_65 0.0942 ms 50.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=32, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_64 0.1027 ms 45.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_68 0.1206 ms 39.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_69 0.1372 ms 34.3% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_66 0.1946 ms 24.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.8759 seconds and 0.0023 seconds precompiling\n",
      "AUTOTUNE convolution(32x128x28x28, 32x128x5x5)\n",
      "  convolution 0.1485 ms 100.0% \n",
      "  triton_convolution2d_77 0.2222 ms 66.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=32, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_72 0.2593 ms 57.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=32, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_74 0.2642 ms 56.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=32, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_71 0.4454 ms 33.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_76 0.4652 ms 31.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_73 0.4874 ms 30.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "  triton_convolution2d_75 0.4916 ms 30.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9421 seconds and 0.0019 seconds precompiling\n",
      "AUTOTUNE convolution(32x128x28x28, 32x128x7x7)\n",
      "  convolution 0.1915 ms 100.0% \n",
      "  triton_convolution2d_84 0.4598 ms 41.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=32, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_79 0.4933 ms 38.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=32, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_81 0.6038 ms 31.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=32, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_80 0.9452 ms 20.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "  triton_convolution2d_83 1.1387 ms 16.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_78 1.1633 ms 16.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_82 1.1858 ms 16.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9872 seconds and 0.0024 seconds precompiling\n",
      "AUTOTUNE convolution(32x128x28x28, 32x128x1x1)\n",
      "  triton_convolution2d_85 0.0246 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_89 0.0246 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_90 0.0246 ms 100.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_88 0.0247 ms 99.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=32, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_91 0.0256 ms 96.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=32, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  convolution 0.0266 ms 92.3% \n",
      "  triton_convolution2d_86 0.0266 ms 92.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=32, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_87 0.0399 ms 61.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.1391 ms 17.7% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9732 seconds and 0.0022 seconds precompiling\n",
      "AUTOTUNE convolution(32x256x14x14, 64x256x3x3)\n",
      "  convolution 0.0602 ms 100.0% \n",
      "  triton_convolution2d_126 0.1536 ms 39.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_124 0.1546 ms 38.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_121 0.1618 ms 37.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_125 0.1679 ms 35.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_127 0.1812 ms 33.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_122 0.2499 ms 24.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_123 0.3123 ms 19.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9018 seconds and 0.0019 seconds precompiling\n",
      "AUTOTUNE convolution(32x256x14x14, 64x256x5x5)\n",
      "  convolution 0.1812 ms 100.0% \n",
      "  triton_convolution2d_131 0.3840 ms 47.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_133 0.4557 ms 39.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_132 0.4577 ms 39.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_134 0.4669 ms 38.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_128 0.4710 ms 38.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_129 0.6616 ms 27.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_130 0.8059 ms 22.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9648 seconds and 0.0020 seconds precompiling\n",
      "AUTOTUNE convolution(32x256x14x14, 64x256x7x7)\n",
      "  convolution 0.1912 ms 100.0% \n",
      "  triton_convolution2d_138 0.7731 ms 24.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_141 0.9349 ms 20.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_140 1.0824 ms 17.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_139 1.0885 ms 17.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_135 1.0988 ms 17.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_136 1.2902 ms 14.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_137 1.5779 ms 12.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.0070 seconds and 0.0021 seconds precompiling\n",
      "AUTOTUNE convolution(32x256x14x14, 64x256x1x1)\n",
      "  convolution 0.0164 ms 100.0% \n",
      "  triton_convolution2d_147 0.0195 ms 84.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_146 0.0206 ms 79.4% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_142 0.0256 ms 64.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_145 0.0256 ms 64.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_143 0.0297 ms 55.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_148 0.0328 ms 50.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_144 0.0543 ms 30.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "  conv1x1_via_mm 0.0766 ms 21.4% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9679 seconds and 0.0026 seconds precompiling\n",
      "AUTOTUNE convolution(32x512x7x7, 128x512x3x3)\n",
      "  convolution 0.0892 ms 100.0% \n",
      "  triton_convolution2d_183 0.2499 ms 35.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_185 0.3359 ms 26.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_184 0.3798 ms 23.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_179 0.3871 ms 23.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_182 0.4526 ms 19.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_180 0.4700 ms 19.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_181 0.6207 ms 14.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.9452 seconds and 0.0031 seconds precompiling\n",
      "AUTOTUNE convolution(32x512x7x7, 128x512x5x5)\n",
      "  convolution 0.1280 ms 100.0% \n",
      "  triton_convolution2d_190 0.6001 ms 21.3% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_192 0.9134 ms 14.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_191 0.9882 ms 13.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_186 1.0465 ms 12.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_189 1.2718 ms 10.1% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_187 1.2853 ms 10.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_188 1.6732 ms 7.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=5, KERNEL_W=5, PADDING_H=2, PADDING_W=2, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.0036 seconds and 0.0021 seconds precompiling\n",
      "AUTOTUNE convolution(32x512x7x7, 128x512x7x7)\n",
      "  convolution 0.3543 ms 100.0% \n",
      "  triton_convolution2d_197 1.3119 ms 27.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_199 1.8135 ms 19.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_198 1.9201 ms 18.5% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_193 2.1905 ms 16.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_194 2.5407 ms 13.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_196 2.5426 ms 13.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_195 3.2461 ms 10.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=7, KERNEL_W=7, PADDING_H=3, PADDING_W=3, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.0628 seconds and 0.0025 seconds precompiling\n",
      "AUTOTUNE convolution(32x512x7x7, 128x512x1x1)\n",
      "  convolution 0.0225 ms 100.0% \n",
      "  triton_convolution2d_204 0.0348 ms 64.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_205 0.0348 ms 64.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_200 0.0368 ms 61.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_203 0.0451 ms 50.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_206 0.0471 ms 47.8% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_201 0.0532 ms 42.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=2, num_warps=4\n",
      "  conv1x1_via_mm 0.0633 ms 35.6% \n",
      "  triton_convolution2d_202 0.0922 ms 24.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=1, KERNEL_W=1, PADDING_H=0, PADDING_W=0, STRIDE_H=1, STRIDE_W=1, UNROLL=True, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.8943 seconds and 0.0036 seconds precompiling\n",
      "AUTOTUNE addmm(32x10, 32x256, 256x10)\n",
      "  triton_mm_256 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2\n",
      "  triton_mm_258 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2\n",
      "  triton_mm_264 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=2\n",
      "  triton_mm_262 0.0084 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=2\n",
      "  triton_mm_257 0.0092 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2\n",
      "  triton_mm_261 0.0092 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=2\n",
      "  triton_mm_263 0.0092 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=2\n",
      "  triton_mm_265 0.0092 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2\n",
      "  triton_mm_260 0.0113 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2\n",
      "  triton_mm_259 0.0133 ms 61.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.3528 seconds and 0.0030 seconds precompiling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预热编译用时157431.0781 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py:2345: UserWarning: Unable to hit fast path of CUDAGraphs because of pending, uninvoked backwards. Consider running with torch.no_grad() or using torch.compiler.cudagraph_mark_step_begin() before each model invocation\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max-autotune模式下编译后的模型前向传播用时12.3890 ms\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "compile_time_list = []\n",
    "compiled_net = torch.compile(complex_net, mode='max-autotune', dynamic=False)\n",
    "start_event.record()\n",
    "_ = compiled_net(input_tensor)  # 预热编译\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "warm_time = start_event.elapsed_time(end_event)\n",
    "print(f\"预热编译用时{warm_time:.4f} ms\")\n",
    "# 下边是default模式下编译后的模型前向传播的用时测试\n",
    "for i in range(n):\n",
    "    start_event.record()\n",
    "    _ = compiled_net(input_tensor)\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "    compile_time = start_event.elapsed_time(end_event)\n",
    "    compile_time_list.append(compile_time)\n",
    "print(f\"max-autotune模式下编译后的模型前向传播用时{sum(compile_time_list) / n:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447da29",
   "metadata": {},
   "source": [
    "忧郁脸……\n",
    "总结一下，按理来说模型编译后的性能理应是max-autotune > reduce-overhead > default，但在这里似乎并不能展示出来这种理论结果。\n",
    "不过这也另一方面展示，其实不同模式的编译效果也略有些不稳定，本人（笨人）刚刚做实验也发现这一点，他们的编译效果是有不小浮动的。\n",
    "\n",
    "算了，我开摆了……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc0075",
   "metadata": {},
   "source": [
    "先不摆了，师兄说他见到的一般是针对函数compile，下边再探索一下针对函数compile和针对整个model compile的效果区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae46dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基准性能 (无编译): 83.72 ms/批次\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTOTUNE convolution(16x2x224x224, 1x2x3x3)\n",
      "  convolution 0.0246 ms 100.0% \n",
      "  triton_convolution2d_2 0.0850 ms 28.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "  triton_convolution2d_1 0.1055 ms 23.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_4 0.1065 ms 23.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_3 0.1137 ms 21.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_0 0.1300 ms 18.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.6647 seconds and 0.7400 seconds precompiling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数级别编译: 75.58 ms/批次\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "AUTOTUNE convolution(16x128x224x224, 256x128x3x3)\n",
      "  convolution 6.8065 ms 100.0% \n",
      "  triton_convolution2d_31 9.6205 ms 70.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_36 11.4135 ms 59.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_33 15.7204 ms 43.3% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_32 19.5881 ms 34.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "  triton_convolution2d_34 23.5182 ms 28.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_35 24.1183 ms 28.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_30 28.7772 ms 23.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.8184 seconds and 0.0010 seconds precompiling\n",
      "AUTOTUNE convolution(16x64x224x224, 128x64x3x3)\n",
      "  convolution 1.8514 ms 100.0% \n",
      "  triton_convolution2d_12 2.5375 ms 73.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_17 2.9235 ms 63.3% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_14 3.9404 ms 47.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_13 4.9551 ms 37.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "  triton_convolution2d_11 6.1389 ms 30.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_16 6.1768 ms 30.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_15 6.4594 ms 28.7% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.1747 seconds and 0.0013 seconds precompiling\n",
      "AUTOTUNE convolution(16x128x224x224, 128x128x3x3)\n",
      "  convolution 3.6353 ms 100.0% \n",
      "  triton_convolution2d_19 4.8712 ms 74.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_24 5.8481 ms 62.2% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_21 8.0732 ms 45.0% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_20 10.1520 ms 35.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "  triton_convolution2d_22 11.7647 ms 30.9% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_18 12.1958 ms 29.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_23 12.2624 ms 29.6% ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.3615 seconds and 0.0011 seconds precompiling\n",
      "AUTOTUNE convolution(16x3x224x224, 64x3x3x3)\n",
      "  triton_convolution2d_5 0.2621 ms 100.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  triton_convolution2d_10 0.2724 ms 96.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_9 0.2725 ms 96.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_8 0.2734 ms 95.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=8\n",
      "  triton_convolution2d_6 0.2796 ms 93.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=2, num_warps=4\n",
      "  convolution 0.2939 ms 89.2% \n",
      "  triton_convolution2d_7 0.3226 ms 81.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=3, KERNEL_W=3, PADDING_H=1, PADDING_W=1, STRIDE_H=1, STRIDE_W=1, UNROLL=False, num_stages=1, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.8201 seconds and 0.0010 seconds precompiling\n",
      "AUTOTUNE addmm(16x10, 16x256, 256x10)\n",
      "  bias_addmm 0.0051 ms 100.0% \n",
      "  addmm 0.0082 ms 62.5% \n",
      "  triton_mm_38 0.0082 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=1\n",
      "  triton_mm_40 0.0082 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=1\n",
      "  triton_mm_43 0.0082 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=1\n",
      "  triton_mm_45 0.0082 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=1\n",
      "  triton_mm_46 0.0082 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=1\n",
      "  triton_mm_47 0.0082 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=1\n",
      "  triton_mm_39 0.0092 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=1\n",
      "  triton_mm_44 0.0092 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=1\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.3612 seconds and 0.0020 seconds precompiling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整个模型编译: 47.04 ms/批次\n",
      "\n",
      "函数级别编译加速: 1.11x\n",
      "整个模型编译加速: 1.78x\n",
      "整个模型相比函数级别额外加速: 1.61x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# 定义一些计算密集的操作，适合单独编译优化\n",
    "def complex_activation(x):\n",
    "    \"\"\"一个计算密集的自定义激活函数\"\"\"\n",
    "    # 使用多个连续操作模拟复杂计算\n",
    "    a = torch.sin(x)\n",
    "    b = torch.pow(x, 2) * torch.tanh(x)\n",
    "    c = torch.exp(-torch.abs(x))\n",
    "    d = torch.sigmoid(x) * torch.log(torch.clamp(x.abs(), min=1e-10))\n",
    "    return a + b - c + d\n",
    "\n",
    "def feature_enhancement(x):\n",
    "    \"\"\"特征增强函数\"\"\"\n",
    "    # 复杂的特征处理\n",
    "    b, c, h, w = x.shape\n",
    "    # 创建通道注意力\n",
    "    channel_avg = F.adaptive_avg_pool2d(x, 1)\n",
    "    channel_weight = torch.sigmoid(channel_avg * 3)\n",
    "    # 空间注意力\n",
    "    spatial_avg = torch.mean(x, dim=1, keepdim=True)\n",
    "    spatial_max, _ = torch.max(x, dim=1, keepdim=True)\n",
    "    spatial_attn = torch.sigmoid(torch.cat([spatial_avg, spatial_max], dim=1))\n",
    "    spatial_weight = torch.sigmoid(F.conv2d(spatial_attn, torch.ones(1, 2, 3, 3).to(x.device), padding=1))\n",
    "    \n",
    "    return x * channel_weight * spatial_weight\n",
    "\n",
    "# 主网络类\n",
    "class LocalOptimizedNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 基础卷积层\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 特征提取块\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # 特征融合块\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # 分类器\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(256, 10)\n",
    "        \n",
    "        # 特征增强函数 - 可以单独编译\n",
    "        self.enhance_features = feature_enhancement\n",
    "        \n",
    "        # 自定义激活 - 可以单独编译\n",
    "        self.activation = complex_activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 初始特征提取\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 特征处理块 1\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation(x)  # 使用计算密集的自定义激活\n",
    "        \n",
    "        # 特征处理块 2\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 特征增强 - 计算密集操作\n",
    "        x = self.enhance_features(x)\n",
    "        \n",
    "        # 最终处理\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 分类\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 测试和比较不同编译策略\n",
    "def test_compilation_strategies():\n",
    "    # 创建模型和测试数据\n",
    "    model = LocalOptimizedNetwork().to(\"cuda\")\n",
    "    x = torch.randn(16, 3, 224, 224).to(\"cuda\")\n",
    "    \n",
    "    # 1. 无编译基准\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    base_time = (time.time() - start) / 10\n",
    "    print(f\"基准性能 (无编译): {base_time*1000:.2f} ms/批次\")\n",
    "    \n",
    "    # 2. 仅编译计算密集函数\n",
    "    model_func_compiled = LocalOptimizedNetwork().to(\"cuda\")\n",
    "    # 单独编译计算密集函数\n",
    "    model_func_compiled.activation = torch.compile(complex_activation, mode='max-autotune')\n",
    "    model_func_compiled.enhance_features = torch.compile(feature_enhancement, mode='max-autotune')\n",
    "    \n",
    "    # 预热\n",
    "    _ = model_func_compiled(x)\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = model_func_compiled(x)\n",
    "    torch.cuda.synchronize()\n",
    "    func_time = (time.time() - start) / 10\n",
    "    print(f\"函数级别编译: {func_time*1000:.2f} ms/批次\")\n",
    "    \n",
    "    # 3. 整个模型编译\n",
    "    model_full = LocalOptimizedNetwork().to(\"cuda\")\n",
    "    model_compiled = torch.compile(model_full, mode='max-autotune')\n",
    "    \n",
    "    # 预热\n",
    "    _ = model_compiled(x)\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = model_compiled(x)\n",
    "    torch.cuda.synchronize()\n",
    "    full_time = (time.time() - start) / 10\n",
    "    print(f\"整个模型编译: {full_time*1000:.2f} ms/批次\")\n",
    "    \n",
    "    # 比较\n",
    "    func_speedup = base_time / func_time\n",
    "    full_speedup = base_time / full_time\n",
    "    print(f\"\\n函数级别编译加速: {func_speedup:.2f}x\")\n",
    "    print(f\"整个模型编译加速: {full_speedup:.2f}x\")\n",
    "    print(f\"整个模型相比函数级别额外加速: {full_speedup/func_speedup:.2f}x\")\n",
    "\n",
    "# 调用此函数即可测试\n",
    "test_compilation_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd696d",
   "metadata": {},
   "source": [
    "这里看起来非常有意思，我们直接对整个模型compile似乎能达到最高性能，而单独针对部分函数进行compile 似乎并不能达到最佳性能增幅，下边我们再尝试不同的例子来验证我们的猜想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de8ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:135: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整个网络编译后的模型前向传播用时5.6893 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取和分类器部分单独编译后的模型前向传播用时5.9003 ms\n",
      "整个网络编译的前向传播用时: 5.6893 ms\n",
      "特征提取和分类器部分单独编译的前向传播用时: 5.9003 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# 下边这个网络是本人（笨人）直接做某个project写出来的网络\n",
    "# 接下来我们尝试两种编译方案，第一种是将整个网络编译，第二种是将网络\n",
    "# 串联的feature和classifier两部分单独编译，我们看一下之后的效果\n",
    "class MixBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MixBlock, self).__init__()\n",
    "        self.channel1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.channel1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.channel2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.channel3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # 第四路也用卷积 + 池化，保证输入输出都是 4D\n",
    "        self.channel4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * 4, out_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.channel1(x)\n",
    "        x2 = self.channel2(x)\n",
    "        x3 = self.channel3(x)\n",
    "        x4 = self.channel4(x)\n",
    "        \n",
    "        # 在通道维度上拼接\n",
    "        x_cat = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        # 融合\n",
    "        x_cat = self.fusion(x_cat)\n",
    "        \n",
    "        return x_cat\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "features = nn.Sequential(\n",
    "            MixBlock(3, 32),  # 输入通道数为3，输出通道数为32\n",
    "            MixBlock(32, 64),  # 输入通道数为32，输出通道数为64\n",
    "            MixBlock(64, 128),  # 输入通道数为64，输出通道数为128\n",
    "        )\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 * 8 * 8, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "class ConvNetMix(nn.Module):\n",
    "    '''\n",
    "    多尺度特征融合\n",
    "    '''\n",
    "    def __init__(self, features, classifier):\n",
    "        super().__init__()\n",
    "        # 特征提取部分\n",
    "        self.features = features\n",
    "        \n",
    "        # 分类器部分\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # 扁平化，保留批次维度\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "# 方案一：整个网络编译\n",
    "net = ConvNetMix(features, classifier).to(\"cuda\")\n",
    "compiled_net = torch.compile(net)\n",
    "# 预热编译\n",
    "input_tensor = torch.randn(32, 3, 64, 64).to(\"cuda\")  # 假设输入是64x64的RGB图像\n",
    "_ = compiled_net(input_tensor)\n",
    "# 测试编译后的模型性能\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "_ = compiled_net(input_tensor)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "inference_time = start_event.elapsed_time(end_event)\n",
    "print(f\"整个网络编译后的模型前向传播用时{inference_time:.4f} ms\")\n",
    "# 方案二：将特征提取和分类器部分单独编译\n",
    "features_compiled = torch.compile(features)\n",
    "classifier_compiled = torch.compile(classifier)\n",
    "# 创建一个新的模型，将编译后的特征提取和分类器组合起来\n",
    "net_separate = ConvNetMix(features_compiled, classifier_compiled).to(\"cuda\")\n",
    "# 预热编译\n",
    "_ = net_separate(input_tensor)\n",
    "# 测试编译后的模型性能\n",
    "start_event.record()\n",
    "_ = net_separate(input_tensor)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "inference_time_separate = start_event.elapsed_time(end_event)\n",
    "print(f\"特征提取和分类器部分单独编译后的模型前向传播用时{inference_time_separate:.4f} ms\")\n",
    "# 比较两种编译方案的性能\n",
    "print(f\"整个网络编译的前向传播用时: {inference_time:.4f} ms\")\n",
    "print(f\"特征提取和分类器部分单独编译的前向传播用时: {inference_time_separate:.4f} ms\")\n",
    "# 这里我们可以看到，整个网络编译的前向传播用时通常会更短，这一点很符合直觉。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdf433",
   "metadata": {},
   "source": [
    "唔……那既然如此，所有情况下都是直接编译全局网络会得到最优性能吗，我们考虑引入多个分支试试，还是上边的网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7fa71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取和分类器部分单独编译后的模型前向传播用时1522.7024 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MixBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MixBlock, self).__init__()\n",
    "        self.channel1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.channel1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.channel2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.channel3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # 第四路也用卷积 + 池化，保证输入输出都是 4D\n",
    "        self.channel4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * 4, out_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.channel1(x)\n",
    "        x2 = self.channel2(x)\n",
    "        x3 = self.channel3(x)\n",
    "        x4 = self.channel4(x)\n",
    "        \n",
    "        # 在通道维度上拼接\n",
    "        x_cat = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        # 融合\n",
    "        x_cat = self.fusion(x_cat)\n",
    "        \n",
    "        return x_cat\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "features = nn.Sequential(\n",
    "            MixBlock(3, 32),  # 输入通道数为3，输出通道数为32\n",
    "            MixBlock(32, 64),  # 输入通道数为32，输出通道数为64\n",
    "            MixBlock(64, 128),  # 输入通道数为64，输出通道数为128\n",
    "        )\n",
    "\n",
    "\n",
    "classifier_list = nn.ModuleList([nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 * 8 * 8, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "            ) for _ in range(5)])  \n",
    "# 创建多个分类器，但最后只会使用一个\n",
    "\n",
    "\n",
    "class ConvNetMix(nn.Module):\n",
    "    '''\n",
    "    多尺度特征融合\n",
    "    '''\n",
    "    def __init__(self, features, classifier_list):\n",
    "        super().__init__()\n",
    "        # 特征提取部分\n",
    "        self.features = features\n",
    "        \n",
    "        # 分类器部分\n",
    "        self.classifiers = classifier_list\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # 扁平化，保留批次维度\n",
    "        x = self.classifiers[np.random.randint(0, len(self.classifiers))](x)  # 随机选择一个分类器\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "'''\n",
    "# 方案一：整个网络编译\n",
    "net = ConvNetMix(features, classifier_list).to(\"cuda\")\n",
    "compiled_net = torch.compile(net, dynamic=True, mode='reduce-overhead')\n",
    "# 预热编译\n",
    "input_tensor = torch.randn(32, 3, 64, 64).to(\"cuda\")  # 假设输入是64x64的RGB图像\n",
    "_ = compiled_net(input_tensor)\n",
    "# 测试编译后的模型性能\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "_ = compiled_net(input_tensor)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "inference_time = start_event.elapsed_time(end_event)\n",
    "print(f\"整个网络编译后的模型前向传播用时{inference_time:.4f} ms\")\n",
    "'''\n",
    "# 方案二：将特征提取和分类器部分单独编译\n",
    "features_compiled = torch.compile(features)\n",
    "classifier_compiled = nn.ModuleList([torch.compile(classifier) for classifier in classifier_list])\n",
    "# 创建一个新的模型，将编译后的特征提取和分类器组合起来\n",
    "net_separate = ConvNetMix(features_compiled, classifier_compiled).to(\"cuda\")\n",
    "# 预热编译\n",
    "_ = net_separate(input_tensor)\n",
    "# 测试编译后的模型性能\n",
    "start_event.record()\n",
    "_ = net_separate(input_tensor)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()  # 等待所有CUDA操作完成\n",
    "inference_time_separate = start_event.elapsed_time(end_event)\n",
    "print(f\"特征提取和分类器部分单独编译后的模型前向传播用时{inference_time_separate:.4f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aca10a",
   "metadata": {},
   "source": [
    "这里发现了一个很有意思的地方，我丧心病狂的用numpy随机数决定我下游使用什么分类器，然后在我直接对整个网络进行编译的时候————编译失败了！\n",
    "也就是说，尽管整个网络进行编译，更多时候能达到最佳性能，但是当网络内部包含复杂的控制流等逻辑的时候，似乎更可能导致编译失败，所以函数级别，小网络级别的编译，虽然可能达不到最佳性能，但是更灵活更稳定！\n",
    "\n",
    "一些个人分析：\n",
    "当网络包含复杂的控制流的时候，可能会涉及一些非torch原生的操作，编译后端可能不能很好的处理控制节点上下游的网络的编译优化，因而在这里会容易出现编译失败。但如果单独编译节点上下游的网络，一般就更容易编译成功"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22042cb8",
   "metadata": {},
   "source": [
    "这下可以开摆了~\n",
    "有误请[指正](mailto:12410615@mail.sustech.edu.cn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
